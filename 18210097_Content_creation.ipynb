{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Translation Evaluation using BLEU score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What is machine translation(MT)?\n",
    "\n",
    "In simple terms, translation of text(or speech) in one language to a different language is called Machine translation.\n",
    "\n",
    "The most basic method of MT is to convert each word of a sentence in a language to the sentence in the targeted language. \n",
    "Example:\n",
    "English sentence: We are learning machine translation  \n",
    "Hindi sentence: हम मशीन अनुवाद सीख रहे हैं\n",
    "But consider an idiom,\"On the ball\" the word to word translation for this can be \"गेंद पर\" whereas the actual meaning of the sentence in hindi is \"अच्छा काम करना\"\n",
    "\n",
    "What are the alternatives?\n",
    "Apart from simple word-to-word conversion there are other Machine translation methods available such as:\n",
    "\n",
    "1. Rule based\n",
    "2. Statistical\n",
    "3. Hybrid (rule based and statistical)\n",
    "4. Neural Machine translation (most effective).\n",
    "\n",
    "Major challenge!\n",
    "The major challenge is the evaluation of quality of the translated text. There are some evaluation metrics such as BLEU, NIST, METEOR, and LEPOR. BLEU metric is described in detail in the following sections.\n",
    "\n",
    "BLEU Score:\n",
    "\n",
    "This will tell us about the amount of match in the reference document and the hypothesis document. The range for the BLEU score is [0.0, 1.0]. A perfect match will result in the BLEU score of 1.0 whereas the total mismatch will be a BLEU score of 0.0\n",
    "We can find the BLEU score for:\n",
    "1. One or more sentences using sentence_bleu() from nltk\n",
    "2. Entire Corpus using corpus_bleu() from nltk\n",
    "\n",
    "In the next two code exmaples I have explained how we can calculate both the sentence_bleu and corpus_bleu scores respectivelty.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The standard function definition for the sentence_bleu score is:\n",
    "nltk.translate.bleu_score.sentence_bleu(references, hypothesis, weights=(0.25, 0.25, 0.25, 0.25), \n",
    "                                        smoothing_function=None, auto_reweigh=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the reference sentence(s)\n",
      "We are doing machine translation\n",
      "Are you done? (y/n)n\n",
      "Enter the reference sentence(s)\n",
      "We are learning machine translation\n",
      "Are you done? (y/n)n\n",
      "Enter the reference sentence(s)\n",
      "हम मशीन अनुवाद सीख रहे हैं\n",
      "Are you done? (y/n)n\n",
      "Enter the reference sentence(s)\n",
      "Why to learn translation?\n",
      "Are you done? (y/n)y\n",
      "Enter the Hypothesis sentence(s): machine translation सीख लो\n",
      "\n",
      "\n",
      "Reference sentences are:  [['we', 'are', 'doing', 'machine', 'translation'], ['we', 'are', 'learning', 'machine', 'translation'], ['हम', 'मशीन', 'अनुवाद', 'सीख', 'रहे', 'हैं'], ['why', 'to', 'learn', 'translation', '?']]\n",
      "Hypothesis sentences are:  ['machine', 'translation', 'सीख', 'लो']\n",
      "BLEU score is: 8.214546595247418e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivek/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/vivek/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "reference = []\n",
    "while True:\n",
    "    print(\"Enter the reference sentence(s)\")\n",
    "    st=input()\n",
    "    inp = nltk.word_tokenize(st.lower())\n",
    "    reference.append(inp)\n",
    "    q=input(\"Are you done? (y/n)\")\n",
    "    if q.lower()=='y':\n",
    "        break\n",
    "cand=input(\"Enter the Hypothesis sentence(s): \")\n",
    "candidate = nltk.word_tokenize(cand.lower())\n",
    "print(\"\\n\\nReference sentences are: \",reference)\n",
    "print(\"Hypothesis sentences are: \",candidate)\n",
    "weights=(0.25, 0.25, 0.25, 0.25)\n",
    "score = sentence_bleu(reference, candidate,weights)\n",
    "print(\"BLEU score is:\",score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As we can see the above code throws a warning, we will handle this in the coming section."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The standard function definition for corpus_bleu score is:\n",
    "nltk.translate.bleu_score.corpus_bleu(list_of_references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), \n",
    "                                      smoothing_function=None, auto_reweigh=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the reference corpus:\n",
      "I told you machine translation will be really helpful\n",
      "Are you done? (y/n)n\n",
      "Enter the reference corpus:\n",
      "See it turns out to be really interesting.\n",
      "Are you done? (y/n)n\n",
      "Enter the reference corpus:\n",
      "One more test corpus I am writing.\n",
      "Are you done? (y/n)n\n",
      "Enter the reference corpus:\n",
      "It is the time to calculate the BLEU score.\n",
      "Are you done? (y/n)y\n",
      "Enter the Hypothesis corpus: BLEU score cannot be helpful and interesting.\n",
      "\n",
      "\n",
      "Reference corpus:  [[['i', 'told', 'you', 'machine', 'translation', 'will', 'be', 'really', 'helpful'], ['see', 'it', 'turns', 'out', 'to', 'be', 'really', 'interesting', '.'], ['one', 'more', 'test', 'corpus', 'i', 'am', 'writing', '.'], ['it', 'is', 'the', 'time', 'to', 'calculate', 'the', 'bleu', 'score', '.']]]\n",
      "Hypothesis coupus :  [['bleu', 'score', 'can', 'not', 'be', 'helpful', 'and', 'interesting', '.']]\n",
      "BLEU score is: 9.53091075863908e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivek/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/vivek/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import nltk\n",
    "reference = [[]]\n",
    "while True:\n",
    "    print(\"Enter the reference corpus:\")\n",
    "    st=input()\n",
    "    inp = nltk.word_tokenize(st.lower())\n",
    "    temp=[]\n",
    "    for i in inp:\n",
    "        temp.append(str(i))\n",
    "        \n",
    "    reference[0].append(temp)\n",
    "    \n",
    "    q=input(\"Are you done? (y/n)\")\n",
    "    if q.lower()=='y':\n",
    "        break\n",
    "candidate=input(\"Enter the Hypothesis corpus: \")\n",
    "candidate=nltk.word_tokenize(candidate.lower())\n",
    "candidate1=[]\n",
    "candidate1.append(candidate)\n",
    "print(\"\\n\\nReference corpus: \",reference)\n",
    "print(\"Hypothesis coupus : \",candidate1)\n",
    "weights=(0.25, 0.25, 0.25, 0.25)\n",
    "score = corpus_bleu(reference, candidate1,weights)\n",
    "print(\"BLEU score is:\",score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Warning once again! We will handle it very soon. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the above code example, we can also load an entire text corpus as the reference. \n",
    "The default BLEU score calculation method is the sentence_bleu. The reference documents in the corpus_bleu can be thought as the list of the reference sentences in the sentence_bleu. word_tokenize() is used to pre-process the input to both of the models. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tuple weights is managing the proportion of individual n-gram scores in the final BLEU score calculation. So, if we want to calculate the individual n-gram score we can do so by changing the values in the tuple weights.\n",
    "weights=(1,0,0,0) for the individual 1-gram score\n",
    "weights=(0,1,0,0) for the individual 2-gram score\n",
    "weights=(0,0,1,0) for the individual 3-gram score\n",
    "weights=(0,0,0,1) for the individual 4-gram score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the reference sentence(s)\n",
      "Test data coming to your way.\n",
      "Are you done? (y/n)n\n",
      "Enter the reference sentence(s)\n",
      "I said be ready more data is coming.\n",
      "Are you done? (y/n)n\n",
      "Enter the reference sentence(s)\n",
      "Ok the data is over. Enjoy the results.\n",
      "Are you done? (y/n)y\n",
      "Enter the Hypothesis sentence(s): Test can be tough so enjoy the results.\n",
      "\n",
      "\n",
      "Reference sentences are:  [['test', 'data', 'coming', 'to', 'your', 'way', '.'], ['i', 'said', 'be', 'ready', 'more', 'data', 'is', 'coming', '.'], ['ok', 'the', 'data', 'is', 'over', '.', 'enjoy', 'the', 'results', '.']]\n",
      "Hypothesis sentences are:  ['test', 'can', 'be', 'tough', 'so', 'enjoy', 'the', 'results', '.']\n",
      "1-gram BLEU score is: 0.6666666666666666\n",
      "2-gram BLEU score is: 0.375\n",
      "3-gram BLEU score is: 0.2857142857142857\n",
      "4-gram BLEU score is: 0.16666666666666669\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "reference = []\n",
    "while True:\n",
    "    print(\"Enter the reference sentence(s)\")\n",
    "    st=input()\n",
    "    inp = nltk.word_tokenize(st.lower())\n",
    "    reference.append(inp)\n",
    "    q=input(\"Are you done? (y/n)\")\n",
    "    if q.lower()=='y':\n",
    "        break\n",
    "cand=input(\"Enter the Hypothesis sentence(s): \")\n",
    "candidate = nltk.word_tokenize(cand.lower())\n",
    "print(\"\\n\\nReference sentences are: \",reference)\n",
    "print(\"Hypothesis sentences are: \",candidate)\n",
    "weights=(1,0,0,0)\n",
    "score = sentence_bleu(reference, candidate,weights)\n",
    "print(\"1-gram BLEU score is:\",score)\n",
    "weights=(0,1,0,0)\n",
    "score = sentence_bleu(reference, candidate,weights)\n",
    "print(\"2-gram BLEU score is:\",score)\n",
    "weights=(0,0,1,0)\n",
    "score = sentence_bleu(reference, candidate,weights)\n",
    "print(\"3-gram BLEU score is:\",score)\n",
    "weights=(0,0,0,1)\n",
    "score = sentence_bleu(reference, candidate,weights)\n",
    "print(\"4-gram BLEU score is:\",score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cumulative n-gram score is calculated by giving equal weightage to all the n-grams where n ranges from 1 to n.\n",
    "For Eg.:\n",
    "Cumulative weight for 1-gram:(1,0,0,0)\n",
    "Cumulative weight for 2-gram:(0.5,0.5,0,0)\n",
    "Cumulative weight for 3-gram:(0.33,0.33,0.33,0)\n",
    "Cumulative weight for 4-gram:(0.25,0.25,0.25,0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the reference sentence(s)\n",
      "First test sentence here.\n",
      "Are you done? (y/n)n\n",
      "Enter the reference sentence(s)\n",
      "So we are coming to the end of this tutorial.\n",
      "Are you done? (y/n)n\n",
      "Enter the reference sentence(s)\n",
      "Hope you liked the content!\n",
      "Are you done? (y/n)n\n",
      "Enter the reference sentence(s)\n",
      "Oh I never told you the reasons for the warning!\n",
      "Are you done? (y/n)n\n",
      "Enter the reference sentence(s)\n",
      "Lets reveal the truth!\n",
      "Are you done? (y/n)y\n",
      "Enter the Hypothesis sentence(s): Truth never comes with a warning!\n",
      "\n",
      "\n",
      "Reference sentences are:  [['first', 'test', 'sentence', 'here', '.'], ['so', 'we', 'are', 'coming', 'to', 'the', 'end', 'of', 'this', 'tutorial', '.'], ['hope', 'you', 'liked', 'the', 'content', '!'], ['oh', 'i', 'never', 'told', 'you', 'the', 'reasons', 'for', 'the', 'warning', '!'], ['lets', 'reveal', 'the', 'truth', '!']]\n",
      "Hypothesis sentences are:  ['truth', 'never', 'comes', 'with', 'a', 'warning', '!']\n",
      "Cumulative 1-gram BLEU score is: 0.5714285714285714\n",
      "Cumulative 2-gram BLEU score is: 0.3086066999241838\n",
      "Cumulative 3-gram BLEU score is: 1.3728756229492927e-102\n",
      "Cumulative 4-gram BLEU score is: 8.286571670851008e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivek/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/vivek/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "reference = []\n",
    "while True:\n",
    "    print(\"Enter the reference sentence(s)\")\n",
    "    st=input()\n",
    "    inp = nltk.word_tokenize(st.lower())\n",
    "    reference.append(inp)\n",
    "    q=input(\"Are you done? (y/n)\")\n",
    "    if q.lower()=='y':\n",
    "        break\n",
    "cand=input(\"Enter the Hypothesis sentence(s): \")\n",
    "candidate = nltk.word_tokenize(cand.lower())\n",
    "print(\"\\n\\nReference sentences are: \",reference)\n",
    "print(\"Hypothesis sentences are: \",candidate)\n",
    "weights=(1,0,0,0)\n",
    "score = sentence_bleu(reference, candidate,weights)\n",
    "print(\"Cumulative 1-gram BLEU score is:\",score)\n",
    "weights=(0.5,0.5,0,0)\n",
    "score = sentence_bleu(reference, candidate,weights)\n",
    "print(\"Cumulative 2-gram BLEU score is:\",score)\n",
    "weights=(0.33,0.33,0.33,0)\n",
    "score = sentence_bleu(reference, candidate,weights)\n",
    "print(\"Cumulative 3-gram BLEU score is:\",score)\n",
    "weights=(0.25,0.25,0.25,0.25)\n",
    "score = sentence_bleu(reference, candidate,weights)\n",
    "print(\"Cumulative 4-gram BLEU score is:\",score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To handle the various exceptions (e.g. all the above codes will throw a warning when the BLEU sore is very less) in BLEU score calculation, we can use various methods of SmoothingFunction class. There are a total of 7 methods available in this class. \n",
    "\n",
    "Let us see some examples of BLEU score calculation using smoothing. The below is an example where the code is throwing a warning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.97486269044271e-232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivek/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/vivek/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/vivek/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "smooth_func = SmoothingFunction()\n",
    "\n",
    "print(sentence_bleu([[\"Hi\", \"this\" ,\"is\", \"a\", \"demo\", \"sentence\"]],[\"Hello\", \"there\" ,\"was\", \"no\", \"demo\"]))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As the BLEU score in the above example is very less, we are getting a warning to use SmoothingFunction(). Lets run the above code with method1 from SmoothingFunction class which add epsilon counts to precision with 0 counts to handle this warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0439891724758422\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "smooth_func = SmoothingFunction()\n",
    "\n",
    "print(sentence_bleu([[\"Hi\", \"this\" ,\"is\", \"a\", \"demo\", \"sentence\"]],[\"Hello\", \"there\" ,\"was\", \"no\", \"demo\"],smoothing_function=smooth_func.method1))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "References:\n",
    "http://www.aclweb.org/anthology/P02-1040.pdf\n",
    "http://www.nltk.org/_modules/nltk/translate/bleu_score.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
